import os
from google.cloud import storage  # , bigquery
import requests
import json
import pandas as pd
import csv

#  -----------------------------------------------------------------------
#  cloud storage
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "culture_key.json"
storage_client = storage.Client()
# define bucket
bucket = storage_client.get_bucket('culture_data')

# get token
blob = bucket.blob('spotify_credentials.json')
spotify_key = json.loads(blob.download_as_string())

client_id = spotify_key['client_id']
client_secret = spotify_key['client_secret']

#  -----------------------------------------------------------------------
#  getting spotify api token
def get_token():
    CLIENT_ID = client_id
    CLIENT_SECRET = client_secret

    AUTH_URL = 'https://accounts.spotify.com/api/token'

    # POST
    auth_response = requests.post(AUTH_URL, {
        'grant_type': 'client_credentials',
        'client_id': CLIENT_ID,
        'client_secret': CLIENT_SECRET,
    })

    # convert the response to JSON
    auth_response_data = auth_response.json()

    # save the access token
    access_token = auth_response_data['access_token']
    print(access_token)
    return access_token


token = get_token()

# token for header
headers = {
    'Authorization': 'Bearer {token}'.format(token=token)
}


def search_track_id(song, artist):
    try:
        search_endpoint = "https://api.spotify.com/v1/search"
        query = f"track:{song} artist:{artist}"  # no need to encode query !
        params = {'q': query, 'type': 'track'}
        search_id = requests.get(url=search_endpoint,
                                 params=params,
                                 headers=headers)

        data = search_id.json()
        print(data)
        if data['tracks']['items'] == []: # --> if items is empty, rerun search with song only (not song+artist)
            print("second seach with artist only")
            search_endpoint = "https://api.spotify.com/v1/search"
            second_query = f"{song}"
            params = {'q': second_query, 'type': 'track'}
            search_id = requests.get(url=search_endpoint,
                                     params=params,
                                     headers=headers)
            data = search_id.json()
            print(f"data before first result : {data}")
            result = data['tracks']['items'][0]['id']
            print(f'song only result (first if): {result}')
            return result

        elif not data['tracks']['items'] == []:
            print(f"list not empty, looking for data['tracks']['items'][0]['id'] : {data}")
            result = data['tracks']['items'][0]['id']
            print(f'song and artist result: {result}')
            return result

        else:
            result = data['tracks']['items'][0]['id']
            print(f'song and artist result: {result}')
            return result

    except(IndexError, KeyError):
            error_result = 'error'
            return error_result


def search_song_feature(song_id):
    if song_id == 'no data':
        data = 'NaN'
    else:
        feature_endpoint = 'https://api.spotify.com/v1/audio-features/'
        params = {'ids': song_id}
        song_feature = requests.get(url=feature_endpoint,
                                    params=params,
                                    headers=headers)
        data = song_feature.json()
        print(data)
    return data


#  data = search_song_feature('6drMFfp3JXylwgJNWaFhKv') # --> check if function is working.
#  print(data)

#  ------------------------------------------------------------------------
#  data crunch begins
#  downloading data from cloud storage
blob = bucket.blob('MySpotifyData/StreamingHistory0.json')
data0 = json.loads(blob.download_as_string())
file0 = pd.DataFrame.from_records(data0)

blob = bucket.blob('MySpotifyData/StreamingHistory1.json')
data1 = json.loads(blob.download_as_string())
file1 = pd.DataFrame.from_records(data1)

blob = bucket.blob('MySpotifyData/StreamingHistory2.json')
data2 = json.loads(blob.download_as_string())
file2 = pd.DataFrame.from_records(data2)
print("files downloaded")

#  crunching
file = file0.append([file1, file2])
print("files merged")

#  adding custom key
file['text_key'] = file['artistName']+'__'+file['trackName']

#  creating unique song / artist df
#  cleaning
meta_df = file.drop(['endTime', 'msPlayed'], axis=1)

# checkup
print("meta df")
print(meta_df.shape)
print(type(meta_df))

#  removing uniques
meta_df.drop_duplicates(inplace=True)
print("unique song file created")

# checkup after drop duplicates
print("meta df--after")
print(meta_df.shape)
print(type(meta_df))

# converting dataframe into list of list
list_of_songs = meta_df.values.tolist()
print(list_of_songs)

#  file = pd.read_csv("no_id_tracks.csv")
#  list_of_songs = file.values.tolist()

counter = 0
for item in list_of_songs:
    print("#"*100)
    #  for each song, search for features
    print(item)
    print(search_track_id(item[1], item[0]))
    result = search_song_feature(search_track_id(item[1], item[0]))

    #  for each feature, append it to song
    list_of_features_to_append = ["id", "danceability", "energy", "key", "loudness",
                                  "mode", "speechiness", "acousticness", "instrumentalness",
                                  "liveness", "valence", "tempo", "type", "duration_ms"]
    for feature in list_of_features_to_append:
        try:
            item.append(result['audio_features'][0][feature])
        except(TypeError, KeyError):
            # looking for right id again
            new_result = {'audio_features': [{'danceability': 0, 'energy': 0, 'key': 0, 'loudness': 0, 'mode': 0, 'speechiness': 0, 'acousticness': 0, 'instrumentalness': 0, 'liveness': 0, 'valence': 0, 'tempo': 0, 'type': 'audio_features', 'id': 'no-id', 'uri': 'spotify:track:no-id', 'track_href': 'https://api.spotify.com/v1/tracks/no-id', 'analysis_url': 'https://api.spotify.com/v1/audio-analysis/no-id', 'duration_ms': 0, 'time_signature': 0}]}
            item.append(new_result['audio_features'][0][feature])
            print('exceptionnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn')
            print(new_result)


    #  counter
    counter += 1
    print(f"{round((counter / 5004)*100, 4)}% done")


with open('enriched_songs.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f, quoting=csv.QUOTE_NONNUMERIC)
    header = ['artistName', 'trackName', 'text_key', 'trackId', 'danceability',
              'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',
              'instrumentalness', 'liveness', 'valence', 'tempo', 'type', 'duration_ms']
    writer.writerow(header)
    writer.writerows(list_of_songs)


# uploading new version of csv to gcs
blob = bucket.blob('enriched_songs.csv')
blob.upload_from_filename('enriched_songs.csv')
print("file uploaded into Gcs")
